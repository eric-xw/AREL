{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251000\n",
      "50200\n",
      "9837\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "base_path = \"../VIST/sis\"\n",
    "train_data = json.load(open(osp.join(base_path, \"train.story-in-sequence.json\")))\n",
    "val_data = json.load(open(osp.join(base_path, \"val.story-in-sequence.json\")))\n",
    "test_data = json.load(open(osp.join(base_path, \"test.story-in-sequence.json\")))\n",
    "\n",
    "prefix = [\"train\", \"val\", \"test\"]\n",
    "whole_album2im = {}\n",
    "for i, data in enumerate([train_data, val_data, test_data]):\n",
    "    album2im = {}\n",
    "    for im in data['images']:\n",
    "        if im['album_id'] not in album2im:\n",
    "            album2im[im['album_id']] = [im['id']]\n",
    "        else:\n",
    "            if im['id'] not in album2im[im['album_id']]:\n",
    "                album2im[im['album_id']].append(im['id'])\n",
    "    whole_album2im[prefix[i]] = album2im\n",
    "\n",
    "whole_album = {}\n",
    "story_lines = {}\n",
    "whole_lines = {}\n",
    "story_line_count = 0\n",
    "whole_line_count = 0\n",
    "for i, data in enumerate([train_data, val_data, test_data]):\n",
    "    album_mapping = {}\n",
    "    for annot_new in data[\"annotations\"]:\n",
    "        annot = annot_new[0]\n",
    "        assert len(annot_new) == 1\n",
    "        text = annot['text'].encode('utf8')\n",
    "        if annot['story_id'] not in album_mapping:\n",
    "            album_mapping[annot['story_id']] = {\"text_index\": [story_line_count], \"flickr_id\": [annot['photo_flickr_id']], \"length\": 1, \n",
    "                                                \"album_id\": annot['album_id'], \"album_flickr_id\": whole_album2im[prefix[i]][annot['album_id']],\n",
    "                                                \"whole_text_index\": whole_line_count, \"origin_text\": text}\n",
    "            story_lines[annot['story_id']] = [{\"index\": story_line_count, \"text\": text.split()}]\n",
    "            whole_lines[annot['story_id']] = {\"index\": whole_line_count, \"text\": text.split()}\n",
    "            whole_line_count +=1\n",
    "        else:\n",
    "            album_mapping[annot['story_id']][\"text_index\"].append(story_line_count)\n",
    "            album_mapping[annot['story_id']][\"flickr_id\"].append(annot['photo_flickr_id'])\n",
    "            album_mapping[annot['story_id']][\"length\"] += 1\n",
    "            story_lines[annot['story_id']].append({\"index\": story_line_count, \"text\": text.split()})\n",
    "            whole_lines[annot['story_id']][\"text\"].extend(text.split())\n",
    "            album_mapping[annot['story_id']][\"origin_text\"] += \" \" + text \n",
    "        story_line_count += 1\n",
    "    whole_album[prefix[i]] = album_mapping\n",
    "\n",
    "new_story_lines = [] \n",
    "for l in story_lines.values():\n",
    "    for li in l:\n",
    "        new_story_lines.append(li)\n",
    "story_lines = new_story_lines\n",
    "whole_lines = whole_lines.values()\n",
    "\n",
    "story_lines = [r['text'] for r in sorted(story_lines, key=lambda thing: thing['index'])]\n",
    "whole_lines = [r['text'] for r in sorted(whole_lines, key=lambda thing: thing['index'])]\n",
    "\n",
    "print len(story_lines)\n",
    "print len(whole_lines)\n",
    "\n",
    "from collections import Counter\n",
    "import numpy\n",
    "cnt = Counter()\n",
    "for l in story_lines:\n",
    "    words = l\n",
    "    for w in words:\n",
    "        cnt[w] += 1\n",
    "words2id = {}\n",
    "idx = 2\n",
    "for k, v in cnt.most_common():\n",
    "    if v > 5:\n",
    "        words2id[k] = idx\n",
    "        idx += 1\n",
    "words2id[\"<EOS>\"] = 0\n",
    "words2id[\"<UNK>\"] = 1\n",
    "id2words = {v:k for k,v in words2id.iteritems()}\n",
    "print len(id2words)\n",
    "\n",
    "whole_album[\"words2id\"] = words2id\n",
    "whole_album[\"id2words\"] = {v:k for k,v in words2id.iteritems()}\n",
    "\n",
    "id_story_lines = []\n",
    "for l in story_lines:\n",
    "    s = [words2id[w] if w in words2id else 1 for w in l]\n",
    "    id_story_lines.append(s)\n",
    "\n",
    "id_whole_lines = []\n",
    "for l in whole_lines:\n",
    "    s = [words2id[w] if w in words2id else 1 for w in l]\n",
    "    id_whole_lines.append(s)\n",
    "\n",
    "new_id_whole_lines = []\n",
    "specify_longest = 105\n",
    "for i in range(len(id_whole_lines)):\n",
    "    cur_len = len(id_whole_lines[i])\n",
    "    if cur_len < specify_longest:\n",
    "        new_id_whole_lines.append(id_whole_lines[i] + [0] * (specify_longest - cur_len))\n",
    "    else:\n",
    "        new_id_whole_lines.append(id_whole_lines[i][:specify_longest-1] + [0])\n",
    "\n",
    "data = numpy.asarray(new_id_whole_lines)\n",
    "import h5py\n",
    "f = h5py.File(\"full_story.h5\", \"w\")\n",
    "f.create_dataset(\"story\", data=data)\n",
    "f.close()\n",
    "\n",
    "new_id_story_lines = []\n",
    "specify_longest = 30\n",
    "for i in range(len(id_story_lines)):\n",
    "    cur_len = len(id_story_lines[i])\n",
    "    if cur_len < specify_longest:\n",
    "        new_id_story_lines.append(id_story_lines[i] + [0] * (specify_longest - cur_len))\n",
    "    else:\n",
    "        new_id_story_lines.append(id_story_lines[i][:specify_longest-1] + [0])\n",
    "\n",
    "data = numpy.asarray(new_id_story_lines, \"int32\")\n",
    "import h5py\n",
    "f = h5py.File(\"story.h5\", \"w\")\n",
    "f.create_dataset(\"story\", data=data)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting 38602\n",
      "deleting 38603\n",
      "deleting 8084\n",
      "deleting 8085\n",
      "deleting 8080\n",
      "deleting 8081\n",
      "deleting 8082\n",
      "deleting 8083\n",
      "deleting 8089\n",
      "deleting 7462\n",
      "deleting 7460\n",
      "deleting 7464\n",
      "deleting 36709\n",
      "deleting 36708\n",
      "deleting 36707\n",
      "deleting 36706\n",
      "deleting 36705\n",
      "deleting 18994\n",
      "deleting 18992\n",
      "deleting 18993\n",
      "deleting 18990\n",
      "deleting 18991\n",
      "deleting 36659\n",
      "deleting 36658\n",
      "deleting 36655\n",
      "deleting 36657\n",
      "deleting 36656\n",
      "deleting 8053\n",
      "deleting 8052\n",
      "deleting 8051\n",
      "deleting 8050\n",
      "deleting 8054\n",
      "deleting 36756\n",
      "deleting 36757\n",
      "deleting 36758\n",
      "deleting 8020\n",
      "deleting 8024\n",
      "deleting 24331\n",
      "deleting 24330\n",
      "deleting 24333\n",
      "deleting 24332\n",
      "deleting 24334\n",
      "deleting 18610\n",
      "deleting 18613\n",
      "deleting 25587\n",
      "deleting 25586\n",
      "deleting 25588\n",
      "deleting 33058\n",
      "deleting 33059\n",
      "deleting 33055\n",
      "deleting 33056\n",
      "deleting 33057\n",
      "deleting 7388\n",
      "deleting 7389\n",
      "deleting 7387\n",
      "deleting 7385\n",
      "deleting 41915\n",
      "deleting 41919\n",
      "deleting 49042\n",
      "deleting 49044\n",
      "deleting 49043\n",
      "deleting 49040\n",
      "deleting 49041\n"
     ]
    }
   ],
   "source": [
    "for p in prefix:\n",
    "    path = \"/mnt/sshd/wenhuchen/VIST/images_256/{}/\".format(p)\n",
    "    deletables = []\n",
    "    for story_id, story in whole_album[p].iteritems():\n",
    "        d = [osp.exists(osp.join(path, \"{}.jpg\".format(_))) for _ in story[\"flickr_id\"]]\n",
    "        if sum(d) < 5:\n",
    "            print \"deleting {}\".format(story_id)\n",
    "            deletables.append(story_id)\n",
    "        else:\n",
    "            pass\n",
    "    for i in deletables:\n",
    "        del whole_album[p][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flickr_story_map = {}\n",
    "for pre in prefix:\n",
    "    album = whole_album[pre]\n",
    "    for k, v in album.iteritems():\n",
    "        indexes = v['text_index']\n",
    "        for i, flickr_id in enumerate(v['flickr_id']):\n",
    "            if flickr_id not in flickr_story_map:\n",
    "                flickr_story_map[flickr_id] = [indexes[i]]\n",
    "            else:\n",
    "                flickr_story_map[flickr_id].append(indexes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'whole_lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b3501b067f5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlength_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwhole_lines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength_distribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcumulative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'whole_lines' is not defined"
     ]
    }
   ],
   "source": [
    "length_distribution = [len(s) for s in whole_lines]\n",
    "result = plt.hist(length_distribution, bins='auto', cumulative=True, normed=1)\n",
    "#plt.show()\n",
    "length_distribution = [len(s) for s in story_lines]\n",
    "result = plt.hist(length_distribution, bins='auto', cumulative=True, normed=1)\n",
    "#plt.hist(length_distribution, bins='auto')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown words percent is 0.0243577608952\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "\n",
    "base_path = \"../VIST/dii\"\n",
    "train_data = json.load(open(osp.join(base_path, \"train.description-in-isolation.json\")))\n",
    "val_data = json.load(open(osp.join(base_path, \"val.description-in-isolation.json\")))\n",
    "test_data = json.load(open(osp.join(base_path, \"test.description-in-isolation.json\")))\n",
    "\n",
    "mapping = {}\n",
    "mapping_original = {}\n",
    "text_list = []\n",
    "text_list_count = 0\n",
    "unknown_words = 0\n",
    "total_words = 0\n",
    "with_story = 0\n",
    "no_story = 0\n",
    "for i, data in enumerate([train_data, val_data, test_data]):\n",
    "    mapping[prefix[i]] = {}\n",
    "    mapping_original[prefix[i]] = {}\n",
    "    for l in data['annotations']:\n",
    "        if l[0]['photo_flickr_id'] not in mapping[prefix[i]]:\n",
    "            if l[0]['photo_flickr_id'] in flickr_story_map:\n",
    "                stories =  flickr_story_map[l[0]['photo_flickr_id']]\n",
    "            else:\n",
    "                stories = [-1]\n",
    "            mapping[prefix[i]][l[0]['photo_flickr_id']] = {'caption': [text_list_count], 'story': stories}\n",
    "            mapping_original[prefix[i]][l[0]['photo_flickr_id']] = [l[0]['text']]\n",
    "        else:\n",
    "            mapping[prefix[i]][l[0]['photo_flickr_id']]['caption'].append(text_list_count)\n",
    "            mapping_original[prefix[i]][l[0]['photo_flickr_id']].append(l[0]['text'])\n",
    "        text_list_count += 1\n",
    "        assert len(l) == 1\n",
    "        s = []\n",
    "        for w in l[0]['text'].split(\" \"):\n",
    "            if w in words2id:\n",
    "                s.append(words2id[w])  \n",
    "            else:\n",
    "                s.append(1)\n",
    "                unknown_words += 1\n",
    "            total_words += 1\n",
    "        text_list.append(s)\n",
    "print \"unknown words percent is {}\".format(unknown_words / (total_words + 0.0))\n",
    "new_text_list = []\n",
    "specify_longest = 20\n",
    "for i in range(len(text_list)):\n",
    "    cur_len = len(text_list[i])\n",
    "    if cur_len < specify_longest:\n",
    "        new_text_list.append(text_list[i] + [0] * (specify_longest - cur_len))\n",
    "    else:\n",
    "        new_text_list.append(text_list[i][:specify_longest - 1] + [0]) \n",
    "\n",
    "for p in prefix:\n",
    "    path = \"/mnt/sshd/wenhuchen/VIST/images_256/{}/\".format(p)\n",
    "    deletables = []\n",
    "    for flickr_id, story in mapping[p].iteritems():\n",
    "        if not osp.exists(osp.join(path, \"{}.jpg\".format(flickr_id))):\n",
    "            deletables.append(flickr_id)\n",
    "    for i in deletables:\n",
    "        del mapping[p][i]\n",
    "        del mapping_original[p][i]\n",
    "        \n",
    "whole_album[\"image2caption\"] = mapping\n",
    "whole_album[\"image2caption_original\"] = mapping_original\n",
    "\n",
    "with open(\"story_line.json\", 'w') as f:\n",
    "    json.dump(whole_album, f)\n",
    "\n",
    "text_array = numpy.asarray(new_text_list, dtype='int32')\n",
    "import h5py\n",
    "f = h5py.File(\"description.h5\", 'w')\n",
    "f.create_dataset(\"story\", data=text_array)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  36 2293  285   12  141    7   97    5   52 2811    6 1807   14    4\n",
      "   95 1441    2   31   10   90    5   96    3   35    5   79   32    6\n",
      "  264  340  649    2    5   55    3  633 2749    8    3 3920    9    1\n",
      "   32  115    3 8139    8    3  940    2    6  340 1268 3617    9 1716\n",
      "  311  291 1381    2   47  121   81    8  340  649   31   10   85 1278\n",
      "   13 1032    2    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]\n",
      "(251000, 30)\n",
      "9837\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "base_path = \"../VIST/dii\"\n",
    "\n",
    "val_data = json.load(open(osp.join(base_path, \"val.description-in-isolation.json\")))\n",
    "with open(\"val_desc_reference\", \"w\") as f:\n",
    "    for l in val_data['annotations']:\n",
    "        print >> f, \"{}\\t{}\".format(l[0]['photo_flickr_id'], l[0]['text'])\n",
    "\n",
    "import h5py\n",
    "f = h5py.File(\"full_story.h5\", \"r\")\n",
    "print f['story'][0]\n",
    "\n",
    "f = h5py.File(\"story.h5\", \"r\")\n",
    "print f['story'].shape\n",
    "\n",
    "f = open(\"story_line.json\", 'r')\n",
    "data = json.load(f)\n",
    "print len(data['id2words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "\n",
    "zero_fc = numpy.zeros((2048, ), \"float32\")\n",
    "zero_conv = numpy.zeros((2048, 7, 7), \"float32\")\n",
    "\n",
    "train_fc_base = \"/mnt/sshd/xwang/VIST/feature/train/fc\"\n",
    "train_conv_base = \"/mnt/sshd/xwang/VIST/feature/train/conv\"\n",
    "train_name1 = [l.split(\".\")[0] for l in os.listdir(train_fc_base)]\n",
    "\n",
    "train_image_base = \"/mnt/sshd/wenhuchen/VIST/images/train\"\n",
    "train_name2 = [l.split(\".\")[0] for l in os.listdir(train_image_base)]\n",
    "\n",
    "rest = set(train_name2) - set(train_name1)\n",
    "for image in rest:\n",
    "    numpy.save(os.path.join(train_fc_base, \"{}.npy\".format(image)), zero_fc) \n",
    "    numpy.save(os.path.join(train_conv_base, \"{}.npy\".format(image)), zero_conv) \n",
    "\n",
    "val_fc_base = \"/mnt/sshd/xwang/VIST/feature/val/fc\"\n",
    "val_conv_base = \"/mnt/sshd/xwang/VIST/feature/val/conv\"\n",
    "val_name1 = [l.split(\".\")[0] for l in os.listdir(val_fc_base)]\n",
    "\n",
    "val_image_base = \"/mnt/sshd/wenhuchen/VIST/images/val\"\n",
    "val_name2 = [l.split(\".\")[0] for l in os.listdir(val_image_base)]\n",
    "\n",
    "rest = set(val_name2) - set(val_name1)\n",
    "for image in rest:\n",
    "    numpy.save(os.path.join(val_fc_base, \"{}.npy\".format(image)), zero_fc) \n",
    "    numpy.save(os.path.join(val_conv_base, \"{}.npy\".format(image)), zero_conv) \n",
    "\n",
    "test_fc_base = \"/mnt/sshd/xwang/VIST/feature/test/fc\"\n",
    "test_conv_base = \"/mnt/sshd/xwang/VIST/feature/test/conv\"\n",
    "test_name1 = [l.split(\".\")[0] for l in os.listdir(test_fc_base)]\n",
    "\n",
    "test_image_base = \"/mnt/sshd/wenhuchen/VIST/images/test\"\n",
    "test_name2 = [l.split(\".\")[0] for l in os.listdir(test_image_base)]\n",
    "\n",
    "rest = set(test_name2) - set(test_name1)\n",
    "for image in rest:\n",
    "    numpy.save(os.path.join(test_fc_base, \"{}.npy\".format(image)), zero_fc) \n",
    "    numpy.save(os.path.join(test_conv_base, \"{}.npy\".format(image)), zero_conv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40141\n",
      "40098\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"story_line.json\", 'r') as f: \n",
    "    data = json.load(f)\n",
    "\n",
    "print len(data['image2caption']['train'])\n",
    "print len(data['train'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
